Using the wisdom of a crowd to help users satisfy their information needs has been studied before in the literature.
\cite{bernstein2012direct} explored the use of crowdsourcing for offline preparation of answers to tail search queries.
Log mining techniques were used to identify potential question-answer fragment pairs, which were then processed by the crowd to generate the final answer.
This offline procedure allows a search engine to increase the coverage of direct answers to user questions.
In our work, however, the focus is on online question answering, which requires fast responses to the user, who is unlikely to wait more than a minute.
Another related work is targeting a different domain, namely SQL queries.
The CrowdDB system \cite{franklin2011crowddb} is an SQL-like processing system for queries, that cannot be answered by machines only.
In CrowdDB human input is used to collect missing data, perform computationally difficult functions or matching against the query.
In \cite{aydin2014crowdsourcing} authors explored efficient ways to combine human input for multiple choice questions from the ``Who wants to be a millionaire?'' TV show.
In this scenario going with the majority for complex questions isn't effective, and certain answerer confidence weighting schemas can improve the results.  
CrowdSearcher platform of \cite{Bozzon:2012:ASQ:2187836.2187971} proposes to use crowds as a data source in the search process, which connects a searcher with the information available through the users of multiple different social platforms.
In general, such websites open up many opportunities to interact with their users, in particular, identify users who might possess certain knowledge and request it by asking questions.
For example, \cite{nichols2012asking,nichols2013analyzing,mahmud2013recommending} showed that it's possible to get the information about airport security waiting times and product reviews by asking social network users, who identified themselves as being at an airport or mentioned the product of interest correspondingly.
While in this work, we primarily focused on more traditional way of hiring the crowd workers using Amazon Mechanical Turk, integration with social services is an interesting direction for the future work.

Many works have used crowdsourcing to get a valuable information that could guide an automated system for some complex tasks.
For example, entity resolution system of \cite{Whang:2013:QSC:2536336.2536337} asks questions to crowd workers to improve the results accuracy.
Using crowdsourcing for relevance judgments has been studied extensively in the information retrieval community, e.g., \cite{Alonso:2008:CRE:1480506.1480508,alonso2011design,grady2010crowdsourcing} to name a few.
The focus in these works is on document relevance, and the quality of crowdsourced judgments.
Whereas in our paper we are investigating the ability of a crowd to quickly assess the quality of the answers in a nearly real-time setting.
The use of crowdsourcing in IR isn't limited to relevance judgements.
The work of \cite{harris2013comparing} explores crowdsourcing for query formulation task, which could also be used inside an IR-based question answering system.
\cite{lease2013crowdsourcing} provides a good overview of different applications of crowdsourcing in information retrieval.

Crowdsourcing is usually associated with offline data collection, which requires significant amount of time.
Its application to (near) real-time scenarios poses certain additional challenges.
\cite{bernstein2011crowds} introduced the retainer model for recruiting synchronous crowds for interactive real-time tasks and showed their effectiveness on the best single image and creative generation tasks.
VizWiz mobile application of \cite{bigham2010vizwiz} uses a similar strategy to quickly answer visual questions.
This work builds on these ideas and uses the proposed retainer model to integrate a crowd into a real-time question answering system.
The work of \cite{Lasecki:2013:CCC:2501988.2502057} showed how multiple workers can sit behind a conversational agent named Chorus.
Similarly to this work, Chorus used crowd workers to propose and vote on responses to user messages.
However, our CRQA system represents a hybrid approach to question answering, where the automatic QA module proposes certain answer candidates, and workers can judge their quality as well as propose additional responses.
Such an approach allows us to focus on more complex informational questions, for many of which the workers might not know the answer, but still can contribute by estimating the quality of automatically generated candidates.
Chorus, on the contrary, considered somewhat simpler tasks (\textit{e.g.}, things to do in a new place, dinner date ideas, textit{etc.}), but focused more on maintaining a coherent dialog, which poses additional challenges, such as keeping a working dialog memory, keeping the users engaged with the dialog using gamification.
These ideas can be combined with the ideas of our work to build more intelligent assistants, that do not rely completely on the expertise of the workers.
Another use of a crowd for maintaining a dialog is presented in \cite{Bessho:2012:DSU:2392800.2392841}, who let the crowd handle difficult cases, when a system was not able to automatically retrieve a good response from the database of twitter data.
In our paper, we focus on a single part of the human-computer dialog, i.e. question answering, which requires a system to provide some useful information in a response to the user.